{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code block has been run and the Maze class is now available for use.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code block has been run and the QLearningAgent class is now available for use.\n",
      "The reward system has been defined.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from global_land_mask import globe\n",
    "import os\n",
    "from netCDF4 import Dataset\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import product\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.neighbors import KDTree\n",
    "from IPython.display import clear_output\n",
    "import gc\n",
    "\n",
    "# Constants for wind and solar calculations\n",
    "SOLAR_PANEL_AREA = 3000  # m^2\n",
    "SOLAR_PANEL_EFFICIENCY = 0.1726\n",
    "RHO_AIR = 1.225  # kg/m^3, air density at sea level\n",
    "KITE_AREA = 500  # m^2\n",
    "LIFT_COEFFICIENT = 1.0\n",
    "DRAG_COEFFICIENT = 0.1\n",
    "BASE_SHIP_SPEED = 10.29  # m/s (default speed, but will be variable)\n",
    "SECONDS_IN_HOUR = 3600  # seconds\n",
    "\n",
    "# Grid size in degrees (approximately 1 hour travel distance at ship speed)\n",
    "GRID_SIZE_DEGREES = BASE_SHIP_SPEED * SECONDS_IN_HOUR / 444000  # 1 degree ~ 111 km\n",
    "\n",
    "# Directories for wind and solar data\n",
    "WIND_DATA_DIR = \"mera\"\n",
    "SOLAR_DATA_DIR = \"solar energy\"\n",
    "\n",
    "action_tuples = [(-4, 0), (4, 0), (0, -4), (0, 4), (4, 4), (4, -4), (-4, 4), (-4, -4)]\n",
    "actions = {i: action for i, action in enumerate(action_tuples)}\n",
    "\n",
    "# Load MERRA-2 wind data from a NetCDF file\n",
    "def load_wind_data(date):\n",
    "    filename = f\"MERRA2_400.tavg1_2d_flx_Nx.{date.strftime('%Y%m%d')}.nc4.nc4\"\n",
    "    filepath = os.path.join(WIND_DATA_DIR, filename)\n",
    "    dataset = Dataset(filepath)\n",
    "    uwind = dataset.variables['ULML'][:]\n",
    "    vwind = dataset.variables['VLML'][:]\n",
    "    return uwind, vwind\n",
    "\n",
    "# Load MERRA-2 solar data from a NetCDF file\n",
    "def load_solar_data(date):\n",
    "    filename = f\"MERRA2_400.tavg1_2d_rad_Nx.{date.strftime('%Y%m%d')}.nc4_2.nc4\"\n",
    "    filepath = os.path.join(SOLAR_DATA_DIR, filename)\n",
    "    dataset = Dataset(filepath)\n",
    "    swgnt = dataset.variables['SWGNT'][:]\n",
    "    return swgnt\n",
    "\n",
    "def calculate_ship_direction(action):\n",
    "    delta_y, delta_x = action\n",
    "    direction = np.arctan2(delta_y, delta_x)\n",
    "    return direction\n",
    "\n",
    "# Calculate the apparent wind speed and direction\n",
    "def calculate_apparent_wind(uwind, vwind, ship_speed, ship_direction):\n",
    "    wind_speed = np.sqrt(uwind**2 + vwind**2)\n",
    "    wind_direction = np.arctan2(vwind, uwind)\n",
    "    apparent_wind_speed = np.sqrt((wind_speed * np.cos(wind_direction - ship_direction) - ship_speed)**2 + (wind_speed * np.sin(wind_direction - ship_direction))**2)\n",
    "    apparent_wind_direction = np.arctan2(wind_speed * np.sin(wind_direction - ship_direction), wind_speed * np.cos(wind_direction - ship_direction) - ship_speed)\n",
    "    return apparent_wind_speed, apparent_wind_direction\n",
    "\n",
    "# Calculate the thrust from the kite\n",
    "def calculate_kite_thrust(apparent_wind_speed, apparent_wind_direction, ship_direction, kite_area, lift_coefficient, drag_coefficient):\n",
    "    # Calculate the angle difference between the wind direction and the ship direction\n",
    "    angle_diff = np.abs(apparent_wind_direction - ship_direction)\n",
    "    \n",
    "    # Normalize the angle difference to be within [0, π]\n",
    "    angle_diff = np.minimum(angle_diff, 2 * np.pi - angle_diff)\n",
    "    \n",
    "    # Calculate lift and drag forces\n",
    "    lift_force = 0.5 * RHO_AIR * apparent_wind_speed**2 * kite_area * lift_coefficient\n",
    "    drag_force = 0.5 * RHO_AIR * apparent_wind_speed**2 * kite_area * drag_coefficient\n",
    "    \n",
    "    # Determine the thrust effect based on the alignment of wind and ship direction\n",
    "    thrust_effect = np.cos(angle_diff)\n",
    "    thrust_effect[thrust_effect < 0] = -1\n",
    "    thrust_effect[thrust_effect >= 0] = 1\n",
    "    \n",
    "    # Calculate effective thrust\n",
    "    effective_thrust = lift_force * thrust_effect - drag_force\n",
    "    return effective_thrust\n",
    "\n",
    "# Estimate solar power output\n",
    "def calculate_solar_power_output(solar_panel_area, solar_panel_efficiency, swgnt):\n",
    "    power_output = solar_panel_area * solar_panel_efficiency * swgnt\n",
    "    return power_output\n",
    "\n",
    "# Calculate the power generated by the kite system\n",
    "def calculate_kite_power(apparent_wind_speed, apparent_wind_direction, ship_heading, kite_area, lift_coefficient, drag_coefficient):\n",
    "    # Power generated by the kite system\n",
    "    lift_force = 0.5 * RHO_AIR * apparent_wind_speed**2 * kite_area * lift_coefficient\n",
    "    drag_force = 0.5 * RHO_AIR * apparent_wind_speed**2 * kite_area * drag_coefficient\n",
    "\n",
    "    # Effective thrust component\n",
    "    angle_of_attack = apparent_wind_direction - ship_heading\n",
    "    effective_thrust = lift_force * np.cos(angle_of_attack) - drag_force * np.sin(angle_of_attack)\n",
    "\n",
    "    # Power output: zero if effective thrust is negative\n",
    "    power_generated = np.where(effective_thrust > 0, effective_thrust * apparent_wind_speed, 0)\n",
    "\n",
    "    return power_generated\n",
    "\n",
    "# Calculate the Haversine distance between two points on the Earth\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Radius of the Earth in kilometers\n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dphi / 2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "class Maze:\n",
    "    def __init__(self, maze, start_position, goal_position, height, width, lat, lon):\n",
    "        self.maze = maze\n",
    "        self.maze_height = height\n",
    "        self.maze_width = width\n",
    "        self.start_position = start_position\n",
    "        self.goal_position = goal_position\n",
    "        self.lat = lat\n",
    "        self.lon = lon\n",
    "\n",
    "    def show_maze(self):\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(self.maze, cmap='gray')\n",
    "        plt.text(self.start_position[0], self.start_position[1], 'S', ha='center', va='center', color='red', fontsize=20)\n",
    "        plt.text(self.goal_position[0], self.goal_position[1], 'G', ha='center', va='center', color='green', fontsize=20)\n",
    "        plt.xticks([]), plt.yticks([])\n",
    "        plt.show()\n",
    "        np.save(\"maze.npy\", self.maze)\n",
    "\n",
    "print(\"This code block has been run and the Maze class is now available for use.\")\n",
    "origin = [49.469, 29.362]\n",
    "destination = [58.836, 24.385]\n",
    "if origin[0] > destination[0]:\n",
    "    origin, destination = destination, origin\n",
    "\n",
    "lat = np.arange(19.859, 32.995, GRID_SIZE_DEGREES)\n",
    "lon = np.arange(46.248, 64.881+0.625, GRID_SIZE_DEGREES)\n",
    "lon_grid, lat_grid = np.meshgrid(lon, lat)\n",
    "z = globe.is_land(lat_grid, lon_grid)\n",
    "z = np.array(z[::-1].astype(int))\n",
    "\n",
    "test_lat = lat - origin[1]\n",
    "test_lat = test_lat[::-1]\n",
    "test_lon = lon - origin[0]\n",
    "start_pos_lat = np.where(abs(test_lat) == min(abs(test_lat)))\n",
    "start_pos_lon = np.where(abs(test_lon) == min(abs(test_lon)))\n",
    "startpos = (start_pos_lon[0][0], start_pos_lat[0][0])\n",
    "\n",
    "test_lat = lat - destination[1]\n",
    "test_lat = test_lat[::-1]\n",
    "test_lon = lon - destination[0]\n",
    "destination_pos_lat = np.where(abs(test_lat) == min(abs(test_lat)))\n",
    "destination_pos_lon = np.where(abs(test_lon) == min(abs(test_lon)))\n",
    "destinationpos = (destination_pos_lon[0][0], destination_pos_lat[0][0])\n",
    "lat = lat[::-1]\n",
    "lon = lon[::-1]\n",
    "\n",
    "maze = Maze(z, startpos, destinationpos, len(z), len(z[0]), lat, lon)\n",
    "maze.show_maze()\n",
    "class QLearningAgent:\n",
    "    def __init__(self, maze, learning_rate=0.1, discount_factor=0.99, exploration_start=1.0, exploration_end=0.01, num_episodes=100):\n",
    "        self.q_table = np.zeros((maze.maze_height, maze.maze_width, len(action_tuples)))  # Initialize Q-table with zeros\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_start = exploration_start\n",
    "        self.exploration_end = exploration_end\n",
    "        self.num_episodes = num_episodes\n",
    "\n",
    "    def get_exploration_rate(self, current_episode):\n",
    "        # Gradual decay of exploration rate\n",
    "        return self.exploration_start * (self.exploration_end / self.exploration_start) ** (current_episode / (self.num_episodes / 2))\n",
    "\n",
    "    def get_action(self, state, current_episode):\n",
    "        exploration_rate = self.get_exploration_rate(current_episode)\n",
    "        if np.random.rand() < exploration_rate:\n",
    "            return np.random.randint(len(action_tuples))\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state[1], state[0]])\n",
    "\n",
    "    def update_q_table(self, state, action, next_state, reward):\n",
    "        best_next_action = np.argmax(self.q_table[next_state[1], next_state[0]])\n",
    "        current_q_value = self.q_table[state[1], state[0], action]\n",
    "        new_q_value = current_q_value + self.learning_rate * (reward + self.discount_factor * self.q_table[next_state[1], next_state[0], best_next_action] - current_q_value)\n",
    "        self.q_table[state[1], state[0], action] = new_q_value\n",
    "\n",
    "print(\"This code block has been run and the QLearningAgent class is now available for use.\")\n",
    "goal_reward = 1000000\n",
    "wall_penalty = -200000000\n",
    "energy_consumption_penalty = -100  # Penalty for energy consumption\n",
    "max_steps_per_episode = 3945  # 365 days * 24 hours\n",
    "print(\"The reward system has been defined.\")\n",
    "\n",
    "def finish_episode(agent, maze, current_episode, start_time, train=True, optimize_for=\"energy\"):\n",
    "    current_state = maze.start_position\n",
    "    is_done = False\n",
    "    episode_reward = 0\n",
    "    episode_step = 0\n",
    "    path = [current_state]\n",
    "    solar_fluxes = []\n",
    "    kite_thrusts = []\n",
    "    kite_powers = []\n",
    "\n",
    "    ship_direction = np.pi / 4  # Assuming a fixed ship direction (45 degrees from North)\n",
    "\n",
    "    time_increment = timedelta(hours=1)\n",
    "    current_time = start_time\n",
    "\n",
    "    uwind, vwind = load_wind_data(current_time)\n",
    "    swgnt = load_solar_data(current_time)\n",
    "\n",
    "    while not is_done and episode_step < max_steps_per_episode:\n",
    "        action = agent.get_action(current_state, current_episode)\n",
    "        \n",
    "        # Debugging output to check the action\n",
    "        \n",
    "        if action < 0 or action >= len(actions):\n",
    "            print(f\"Invalid action: {action}\")\n",
    "            break\n",
    "\n",
    "        next_state = (current_state[0] + actions[action][0], current_state[1] + actions[action][1])\n",
    "\n",
    "        if (0 <= next_state[0] < maze.maze_width) and (0 <= next_state[1] < maze.maze_height):\n",
    "            if maze.maze[next_state[1], next_state[0]] == 1:\n",
    "                reward = wall_penalty\n",
    "                next_state = current_state\n",
    "            elif next_state == maze.goal_position:\n",
    "                path.append(next_state)\n",
    "                reward = goal_reward\n",
    "                is_done = True\n",
    "            else:\n",
    "                path.append(next_state)\n",
    "                lat1, lon1 = maze.lat[current_state[1]], maze.lon[current_state[0]]\n",
    "                lat2, lon2 = maze.lat[next_state[1]], maze.lon[next_state[0]]\n",
    "                lat_end, lon_end = maze.lat[maze.goal_position[1]], maze.lon[maze.goal_position[0]]\n",
    "                distance_reward = -0.1 * abs(haversine_distance(lat1, lon1, lat2, lon2)) - abs(haversine_distance(lat1, lon1, lat_end, lon_end))\n",
    "                \n",
    "                # Calculate wind and solar effects\n",
    "                hour_index = current_time.hour\n",
    "                apparent_wind_speed, apparent_wind_direction = calculate_apparent_wind(uwind[hour_index], vwind[hour_index], BASE_SHIP_SPEED, np.arctan2(lon2 - lon1, lat2 - lat1))\n",
    "                effective_thrust = calculate_kite_thrust(apparent_wind_speed, apparent_wind_direction, ship_direction, KITE_AREA, LIFT_COEFFICIENT, DRAG_COEFFICIENT)\n",
    "                solar_power_output = calculate_solar_power_output(SOLAR_PANEL_AREA, SOLAR_PANEL_EFFICIENCY, swgnt[hour_index])\n",
    "                \n",
    "                # Calculate wind power (assuming zero if not aligned with ship direction)\n",
    "                wind_power = calculate_kite_power(apparent_wind_speed, apparent_wind_direction, ship_direction, KITE_AREA, LIFT_COEFFICIENT, DRAG_COEFFICIENT)\n",
    "                \n",
    "                # Calculate energy consumption\n",
    "                energy_gain = (effective_thrust.mean() - wind_power.mean() + solar_power_output.mean()) * 0.01  # Adjust scaling as necessary\n",
    "                energy_reward = energy_consumption_penalty * energy_gain\n",
    "                \n",
    "                # Combine rewards\n",
    "                if optimize_for == \"energy\":\n",
    "                    reward = distance_reward + energy_reward\n",
    "                else:\n",
    "                    reward = distance_reward\n",
    "\n",
    "                # Record values for plotting later\n",
    "                solar_fluxes.append(solar_power_output.mean())\n",
    "                kite_thrusts.append(effective_thrust.mean())\n",
    "                kite_powers.append(wind_power.mean())\n",
    "\n",
    "                # Update time and load new data if day changes\n",
    "                current_time += time_increment\n",
    "                if current_time.hour == 0:\n",
    "                    try:\n",
    "                        uwind, vwind = load_wind_data(current_time)\n",
    "                        swgnt = load_solar_data(current_time)\n",
    "                    except FileNotFoundError:\n",
    "                        break  # Stop if data is not available for the next day\n",
    "\n",
    "        else:\n",
    "            reward = wall_penalty\n",
    "            next_state = current_state\n",
    "\n",
    "        episode_reward += reward\n",
    "        episode_step += 1\n",
    "\n",
    "        if train:\n",
    "            agent.update_q_table(current_state, action, next_state, reward)\n",
    "\n",
    "        current_state = next_state\n",
    "\n",
    "    return episode_reward, episode_step, path, solar_fluxes, kite_thrusts, kite_powers\n",
    "\n",
    "def train_agent(agent, maze, start_time, num_episodes=100, optimize_for=\"energy\", high_res_maze=None):\n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "    all_solar_fluxes = []\n",
    "    all_kite_thrusts = []\n",
    "    all_kite_powers = []\n",
    "    all_rewards = []\n",
    "    paths = []\n",
    "    best_path = []\n",
    "    best_solar_fluxes = []\n",
    "    best_kite_thrusts = []\n",
    "    best_kite_powers = []\n",
    "    best_reward = -float('inf')\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        episode_reward, episode_step, path, solar_fluxes, kite_thrusts, kite_powers = finish_episode(agent, maze, episode, start_time, train=True, optimize_for=optimize_for)\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_steps.append(episode_step)\n",
    "        all_solar_fluxes.append(solar_fluxes)\n",
    "        all_kite_thrusts.append(kite_thrusts)\n",
    "        all_kite_powers.append(kite_powers)\n",
    "        all_rewards.append(episode_reward)\n",
    "        paths.append(path)\n",
    "        # Display progress every 100 episodes\n",
    "        print(f\"Episode {episode} - Steps: {episode_step}, Reward: {episode_reward}\")\n",
    "        if episode % 100 == 0:\n",
    "            \n",
    "            clear_output(wait=False)\n",
    "    \n",
    "        if episode_reward > best_reward:\n",
    "            best_reward = episode_reward\n",
    "            best_path = path\n",
    "            best_solar_fluxes = solar_fluxes\n",
    "            best_kite_thrusts = kite_thrusts\n",
    "            best_kite_powers = kite_powers\n",
    "\n",
    "    # Translate the best path into latitude and longitude\n",
    "    best_path_lat_lon = [(maze.lat[pos[1]], maze.lon[pos[0]]) for pos in best_path]\n",
    "\n",
    "    # Use the provided high-resolution maze\n",
    "    new_z = high_res_maze\n",
    "\n",
    "    # Translate best_path_lat_lon into points on the new high-resolution maze\n",
    "    new_lat = np.linspace(min(maze.lat), max(maze.lat), new_z.shape[0])\n",
    "    new_lon = np.linspace(min(maze.lon), max(maze.lon), new_z.shape[1])\n",
    "    \n",
    "    new_best_path = []\n",
    "    for lat, lon in best_path_lat_lon:\n",
    "        # Find the closest latitude index\n",
    "        lat_idx = (np.abs(new_lat - lat)).argmin()\n",
    "\n",
    "        # Find the closest longitude index\n",
    "        lon_idx = (np.abs(new_lon - lon)).argmin()\n",
    "\n",
    "        new_best_path.append((lon_idx, lat_idx))\n",
    "\n",
    "    # Ensure there are no duplicate points\n",
    "    new_best_path = list(dict.fromkeys(new_best_path))\n",
    "\n",
    "    # Sort the new_best_path based on x values (longitudes)\n",
    "    new_best_path = sorted(new_best_path, key=lambda x: x[0])\n",
    "\n",
    "    return episode_rewards, episode_steps, all_solar_fluxes, all_kite_thrusts, all_kite_powers, best_path, best_solar_fluxes, best_kite_thrusts, best_kite_powers, best_reward, new_best_path, new_z\n",
    "\n",
    "\n",
    "start_time = datetime(2023, 1, 1)\n",
    "agent = QLearningAgent(maze, num_episodes=1000)\n",
    "\n",
    "def finish_episode_with_average_data(agent, maze, current_episode, seasonal_data, season, train=True, optimize_for=\"energy\"):\n",
    "    current_state = maze.start_position\n",
    "    is_done = False\n",
    "    episode_reward = 0\n",
    "    episode_step = 0\n",
    "    path = [current_state]\n",
    "    solar_fluxes = []\n",
    "    kite_thrusts = []\n",
    "    kite_powers = []\n",
    "\n",
    "    while not is_done and episode_step < max_steps_per_episode:\n",
    "        hour_index = episode_step % 24  # Reset the hour after 24 hours\n",
    "\n",
    "        uwind = seasonal_data[season]['uwind'][hour_index]\n",
    "        vwind = seasonal_data[season]['vwind'][hour_index]\n",
    "        solar = seasonal_data[season]['solar'][hour_index]\n",
    "\n",
    "        action_index = agent.get_action(current_state, current_episode)\n",
    "        action = actions[action_index]\n",
    "        \n",
    "        # Calculate the ship direction based on the action\n",
    "        ship_direction = calculate_ship_direction(action)\n",
    "        \n",
    "        next_state = (current_state[0] + action[0], current_state[1] + action[1])\n",
    "        \n",
    "        if (0 <= next_state[0] < maze.maze_width) and (0 <= next_state[1] < maze.maze_height):\n",
    "            if maze.maze[next_state[1], next_state[0]] == 1:\n",
    "                reward = wall_penalty\n",
    "                next_state = current_state\n",
    "            elif next_state == maze.goal_position:\n",
    "                path.append(next_state)\n",
    "                reward = goal_reward\n",
    "                is_done = True\n",
    "            else:\n",
    "                path.append(next_state)\n",
    "                lat1, lon1 = maze.lat[current_state[1]], maze.lon[current_state[0]]\n",
    "                lat2, lon2 = maze.lat[next_state[1]], maze.lon[next_state[0]]\n",
    "                lat_end, lon_end = maze.lat[maze.goal_position[1]], maze.lon[maze.goal_position[0]]\n",
    "                distance_reward = -0.1 * abs(haversine_distance(lat1, lon1, lat2, lon2)) - abs(haversine_distance(lat1, lon1, lat_end, lon_end))\n",
    "                \n",
    "                # Calculate wind and solar effects\n",
    "                apparent_wind_speed, apparent_wind_direction = calculate_apparent_wind(uwind, vwind, BASE_SHIP_SPEED, ship_direction)\n",
    "                effective_thrust = calculate_kite_thrust(apparent_wind_speed, apparent_wind_direction, ship_direction, KITE_AREA, LIFT_COEFFICIENT, DRAG_COEFFICIENT)\n",
    "                solar_power_output = calculate_solar_power_output(SOLAR_PANEL_AREA, SOLAR_PANEL_EFFICIENCY, solar)\n",
    "                \n",
    "                # Calculate wind power (assuming zero if not aligned with ship direction)\n",
    "                wind_power = np.where(np.abs(apparent_wind_direction - ship_direction) < np.pi / 2, effective_thrust, 0)\n",
    "                \n",
    "                # Calculate energy consumption\n",
    "                energy_gain = (effective_thrust.mean() - wind_power.mean() + solar_power_output.mean()) * 0.01  # Adjust scaling as necessary\n",
    "                energy_reward = energy_consumption_penalty * energy_gain\n",
    "                \n",
    "                # Combine rewards\n",
    "                if optimize_for == \"energy\":\n",
    "                    reward = distance_reward + energy_reward\n",
    "                else:\n",
    "                    reward = distance_reward\n",
    "\n",
    "                # Record values for plotting later\n",
    "                solar_fluxes.append(solar_power_output.mean())\n",
    "                kite_thrusts.append(effective_thrust.mean())\n",
    "                kite_powers.append(wind_power.mean())\n",
    "        else:\n",
    "            reward = wall_penalty\n",
    "            next_state = current_state\n",
    "\n",
    "        episode_reward += reward\n",
    "        episode_step += 1\n",
    "\n",
    "        if train:\n",
    "            agent.update_q_table(current_state, action_index, next_state, reward)\n",
    "\n",
    "        current_state = next_state\n",
    "\n",
    "    return episode_reward, episode_step, path, solar_fluxes, kite_thrusts, kite_powers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "# Define paths to data directories\n",
    "WIND_DATA_DIR = \"mera\"\n",
    "SOLAR_DATA_DIR = \"solar energy\"\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "\n",
    "# Define seasons as ranges of months\n",
    "seasons = {\n",
    "    'winter': [12, 1, 2],\n",
    "    'spring': [3, 4, 5],\n",
    "    'summer': [6, 7, 8],\n",
    "    'autumn': [9, 10, 11]\n",
    "}\n",
    "\n",
    "# Load MERRA-2 wind data from a NetCDF file\n",
    "def load_wind_data(date):\n",
    "    filename = f\"MERRA2_400.tavg1_2d_flx_Nx.{date.strftime('%Y%m%d')}.nc4.nc4\"\n",
    "    filepath = os.path.join(WIND_DATA_DIR, filename)\n",
    "    dataset = Dataset(filepath)\n",
    "    uwind = dataset.variables['ULML'][:]\n",
    "    vwind = dataset.variables['VLML'][:]\n",
    "    return uwind, vwind\n",
    "\n",
    "# Load MERRA-2 solar data from a NetCDF file\n",
    "def load_solar_data(date):\n",
    "    filename = f\"MERRA2_400.tavg1_2d_rad_Nx.{date.strftime('%Y%m%d')}.nc4_2.nc4\"\n",
    "    filepath = os.path.join(SOLAR_DATA_DIR, filename)\n",
    "    dataset = Dataset(filepath)\n",
    "    swgnt = dataset.variables['SWGNT'][:]\n",
    "    return swgnt\n",
    "\n",
    "# Calculate the average wind and solar data for each season, hour by hour\n",
    "def calculate_hourly_averages_for_season(year):\n",
    "    seasonal_data = {}\n",
    "    \n",
    "    for season, months in seasons.items():\n",
    "        wind_u_hourly_total = np.zeros((24,) + load_wind_data(datetime(year, months[0], 1))[0].shape)\n",
    "        wind_v_hourly_total = np.zeros_like(wind_u_hourly_total)\n",
    "        solar_hourly_total = np.zeros((24,) + load_solar_data(datetime(year, months[0], 1)).shape)\n",
    "        hour_count = np.zeros(24)  # Counter for averaging\n",
    "\n",
    "        for month in months:\n",
    "            num_days = (datetime(year, month % 12 + 1, 1) - datetime(year, month, 1)).days\n",
    "            for day in range(1, num_days + 1):\n",
    "                date = datetime(year, month, day)\n",
    "                try:\n",
    "                    uwind, vwind = load_wind_data(date)\n",
    "                    swgnt = load_solar_data(date)\n",
    "                    \n",
    "                    for hour in range(24):\n",
    "                        wind_u_hourly_total[hour] += uwind[hour]\n",
    "                        wind_v_hourly_total[hour] += vwind[hour]\n",
    "                        solar_hourly_total[hour] += swgnt[hour]\n",
    "                        hour_count[hour] += 1\n",
    "                except FileNotFoundError:\n",
    "                    continue\n",
    "\n",
    "        # Calculate hourly averages\n",
    "        seasonal_data[season] = {\n",
    "            'uwind': wind_u_hourly_total / hour_count[:, None, None],\n",
    "            'vwind': wind_v_hourly_total / hour_count[:, None, None],\n",
    "            'solar': solar_hourly_total / hour_count[:, None, None]\n",
    "        }\n",
    "    \n",
    "    return seasonal_data\n",
    "\n",
    "# finish function\n",
    "def finish_episode_with_hourly_averages(agent, maze, current_episode, seasonal_data, season, train=True, optimize_for=\"energy\"):\n",
    "    current_state = maze.start_position\n",
    "    is_done = False\n",
    "    episode_reward = 0\n",
    "    episode_step = 0\n",
    "    path = [current_state]\n",
    "    solar_fluxes = []\n",
    "    kite_thrusts = []\n",
    "    kite_powers = []\n",
    "\n",
    "    while not is_done and episode_step < max_steps_per_episode:\n",
    "        hour_index = episode_step % 24  # Reset the hour after 24 steps/hours\n",
    "\n",
    "        uwind = seasonal_data[season]['uwind'][hour_index]\n",
    "        vwind = seasonal_data[season]['vwind'][hour_index]\n",
    "        solar = seasonal_data[season]['solar'][hour_index]\n",
    "\n",
    "        action_index = agent.get_action(current_state, current_episode)\n",
    "        action = actions[action_index]\n",
    "\n",
    "        # Calculate the ship direction based on the action\n",
    "        ship_direction = calculate_ship_direction(action)\n",
    "\n",
    "        # Initialize reward as a wall penalty (if a wall is hit)\n",
    "        reward = wall_penalty\n",
    "        valid_move = True\n",
    "\n",
    "        # Check each step incrementally for all 8 possible directions\n",
    "        for step in range(1, 6):  # Check increments of 1, 2, 3, 4, 5\n",
    "            for action_check in action_tuples:\n",
    "                potential_state = (current_state[0] + action_check[0] * step // 5, current_state[1] + action_check[1] * step // 5)\n",
    "                if (0 <= potential_state[0] < maze.maze_width) and (0 <= potential_state[1] < maze.maze_height):\n",
    "                    if maze.maze[potential_state[1], potential_state[0]] == 1:\n",
    "                        valid_move = False\n",
    "                        break\n",
    "                else:\n",
    "                    valid_move = False\n",
    "                    break\n",
    "\n",
    "            if not valid_move:\n",
    "                reward = wall_penalty\n",
    "                break\n",
    "            next_state = (current_state[0] + action[0], current_state[1] + action[1])\n",
    "\n",
    "        if valid_move:\n",
    "            if next_state == maze.goal_position:\n",
    "                path.append(next_state)\n",
    "                reward = goal_reward\n",
    "                is_done = True\n",
    "            else:\n",
    "                path.append(next_state)\n",
    "                lat1, lon1 = maze.lat[current_state[1]], maze.lon[current_state[0]]\n",
    "                lat2, lon2 = maze.lat[next_state[1]], maze.lon[next_state[0]]\n",
    "                lat_end, lon_end = maze.lat[maze.goal_position[1]], maze.lon[maze.goal_position[0]]\n",
    "                distance_reward = -0.1 * abs(haversine_distance(lat1, lon1, lat2, lon2)) - abs(haversine_distance(lat1, lon1, lat_end, lon_end))\n",
    "                \n",
    "                # Calculate wind and solar effects\n",
    "                apparent_wind_speed, apparent_wind_direction = calculate_apparent_wind(uwind, vwind, BASE_SHIP_SPEED, ship_direction)\n",
    "                effective_thrust = calculate_kite_thrust(apparent_wind_speed, apparent_wind_direction, ship_direction, KITE_AREA, LIFT_COEFFICIENT, DRAG_COEFFICIENT)\n",
    "                solar_power_output = calculate_solar_power_output(SOLAR_PANEL_AREA, SOLAR_PANEL_EFFICIENCY, solar)\n",
    "                \n",
    "                # Calculate wind power (assuming zero if not aligned with ship direction)\n",
    "                wind_power = np.where(np.abs(apparent_wind_direction - ship_direction) < np.pi / 2, effective_thrust, 0)\n",
    "                \n",
    "                # Calculate energy consumption\n",
    "                energy_gain = (effective_thrust.mean() - wind_power.mean() + solar_power_output.mean()) * 0.01  # Adjust scaling as necessary\n",
    "                energy_reward = energy_consumption_penalty * energy_gain\n",
    "                \n",
    "                # Combine rewards\n",
    "                if optimize_for == \"energy\":\n",
    "                    reward = distance_reward + energy_reward\n",
    "                else:\n",
    "                    reward = distance_reward\n",
    "\n",
    "                # Record values for plotting later\n",
    "                solar_fluxes.append(solar_power_output.mean())\n",
    "                kite_thrusts.append(effective_thrust.mean())\n",
    "                kite_powers.append(wind_power.mean())\n",
    "        else:\n",
    "            next_state = current_state\n",
    "\n",
    "        episode_reward += reward\n",
    "        episode_step += 1\n",
    "\n",
    "        if train:\n",
    "            agent.update_q_table(current_state, action_index, next_state, reward)\n",
    "\n",
    "        current_state = next_state\n",
    "\n",
    "    return episode_reward, episode_step, path, solar_fluxes, kite_thrusts, kite_powers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode_dqn(agent, env, max_steps_per_episode=1000, train=True):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_step = 0\n",
    "    done = False\n",
    "    path = []\n",
    "\n",
    "    while not done and episode_step < max_steps_per_episode:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        agent.store_transition(state, action, reward, next_state, done)\n",
    "\n",
    "        if train:\n",
    "            agent.train()\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        episode_step += 1\n",
    "        path.append(state)\n",
    "\n",
    "    agent.update_target_model()  # Update target network periodically\n",
    "\n",
    "    return episode_reward, episode_step, path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "class DQN(tf.keras.Model):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = layers.Dense(128, activation='relu')\n",
    "        self.fc2 = layers.Dense(128, activation='relu')\n",
    "        self.fc3 = layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.fc3(x)\n",
    "    \n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_dim, output_dim, lr=0.001, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n",
    "        self.model = DQN(input_dim, output_dim)\n",
    "        self.target_model = DQN(input_dim, output_dim)\n",
    "        self.model.build(input_shape=(None, input_dim))\n",
    "        self.target_model.build(input_shape=(None, input_dim))\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = ReplayMemory(10000)\n",
    "        self.batch_size = 64\n",
    "\n",
    "        self.update_target_model()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(0, self.model.fc3.units)\n",
    "        else:\n",
    "            state = np.expand_dims(state, axis=0).astype(np.float32)\n",
    "            q_values = self.model(state)\n",
    "            return np.argmax(q_values[0])\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
    "\n",
    "        state_batch = np.array(state_batch, dtype=np.float32)\n",
    "        next_state_batch = np.array(next_state_batch, dtype=np.float32)\n",
    "        reward_batch = np.array(reward_batch, dtype=np.float32)\n",
    "        done_batch = np.array(done_batch, dtype=np.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.model(state_batch)\n",
    "            q_values = tf.reduce_sum(q_values * tf.one_hot(action_batch, self.model.fc3.units), axis=1)\n",
    "            next_q_values = self.target_model(next_state_batch)\n",
    "            max_next_q_values = tf.reduce_max(next_q_values, axis=1)\n",
    "            target_q_values = reward_batch + self.gamma * max_next_q_values * (1 - done_batch)\n",
    "            loss = tf.keras.losses.MSE(target_q_values, q_values)\n",
    "\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.memory.push((state, action, reward, next_state, done))\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.save_weights(path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model.load_weights(path)\n",
    "        self.update_target_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class ShipNavigationEnv:\n",
    "    def __init__(self, maze, seasonal_data, season):\n",
    "        self.maze = maze\n",
    "        self.seasonal_data = seasonal_data\n",
    "        self.season = season\n",
    "        self.current_state = self.maze.start_position\n",
    "        self.observation_space = None  # Define this based on your environment\n",
    "        self.action_space = None  # Define this based on your environment\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment to the initial state\n",
    "        self.current_state = self.maze.start_position\n",
    "        \n",
    "        # Define the initial observation based on the starting state\n",
    "        observation = self._get_observation()\n",
    "        \n",
    "        return observation\n",
    "\n",
    "    def _get_observation(self):\n",
    "        # Example observation might include the current position and environmental data\n",
    "        position = self.current_state\n",
    "        wind = self.seasonal_data[self.season]['uwind'][0], self.seasonal_data[self.season]['vwind'][0]\n",
    "        solar = self.seasonal_data[self.season]['solar'][0]\n",
    "        \n",
    "        # Combine all necessary data into a single observation\n",
    "        observation = np.array([position[0], position[1], wind[0], wind[1], solar])\n",
    "        \n",
    "        return observation\n",
    "\n",
    "    def step(self, action):\n",
    "        # Implement the environment's step logic here\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_best_path_and_data(season_dir, season, best_path, best_solar_fluxes, best_kite_thrusts, best_kite_powers, best_reward, episode_rewards, low_res_lat, low_res_lon, high_res_lat, high_res_lon, high_res_maze):\n",
    "    plt.figure(figsize=(38.4, 21.6), dpi=220)  # 4K resolution: 3840x2160 pixels\n",
    "\n",
    "    # Plot 1: Episode rewards\n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.plot(episode_rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title(f'{season.capitalize()} - Reward per Episode')\n",
    "\n",
    "    # Plot 2: Steps per episode (constant for best path)\n",
    "    plt.subplot(3, 2, 2)\n",
    "    plt.plot([len(best_path)] * len(episode_rewards))\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Steps Taken')\n",
    "    plt.title(f'{season.capitalize()} - Steps per Episode')\n",
    "\n",
    "    # Plot 3: High-resolution maze with best path\n",
    "    plt.subplot(3, 2, 3)\n",
    "    plt.imshow(high_res_maze, cmap='gray')\n",
    "\n",
    "    if best_path is not None:\n",
    "        # Translate low-resolution path to latitude and longitude\n",
    "        translated_path = []\n",
    "        for point in best_path:\n",
    "            lat = low_res_lat[point[1]]\n",
    "            lon = low_res_lon[point[0]]\n",
    "            translated_path.append((lat, lon))\n",
    "        \n",
    "        # Find the closest points on the high-resolution maze\n",
    "        high_res_path = []\n",
    "        for lat, lon in translated_path:\n",
    "            lat_diff = np.abs(high_res_lat - lat)\n",
    "            lon_diff = np.abs(high_res_lon - lon)\n",
    "            closest_lat_idx = np.argmin(lat_diff)\n",
    "            closest_lon_idx = np.argmin(lon_diff)\n",
    "            high_res_path.append((closest_lon_idx, closest_lat_idx))\n",
    "\n",
    "        high_res_path_x, high_res_path_y = zip(*high_res_path)\n",
    "        \n",
    "        # Plot the translated path with a thick line\n",
    "        plt.plot(high_res_path_x, high_res_path_y, color='blue', linewidth=3)  # Thicker line for path\n",
    "\n",
    "        # Plot the start and goal positions\n",
    "        start_lat = low_res_lat[maze.start_position[1]]\n",
    "        start_lon = low_res_lon[maze.start_position[0]]\n",
    "        start_lat_diff = np.abs(high_res_lat - start_lat)\n",
    "        start_lon_diff = np.abs(high_res_lon - start_lon)\n",
    "        start_closest_lat_idx = np.argmin(start_lat_diff)\n",
    "        start_closest_lon_idx = np.argmin(start_lon_diff)\n",
    "        plt.scatter([start_closest_lon_idx], [start_closest_lat_idx], color='red', s=100, label='Start')\n",
    "\n",
    "        goal_lat = low_res_lat[maze.goal_position[1]]\n",
    "        goal_lon = low_res_lon[maze.goal_position[0]]\n",
    "        goal_lat_diff = np.abs(high_res_lat - goal_lat)\n",
    "        goal_lon_diff = np.abs(high_res_lon - goal_lon)\n",
    "        goal_closest_lat_idx = np.argmin(goal_lat_diff)\n",
    "        goal_closest_lon_idx = np.argmin(goal_lon_diff)\n",
    "        plt.scatter([goal_closest_lon_idx], [goal_closest_lat_idx], color='green', s=100, label='Goal')\n",
    "\n",
    "        plt.xticks([]), plt.yticks([])\n",
    "        plt.grid(color='black', linewidth=2)\n",
    "        plt.title(f'{season.capitalize()} - Final Path Learned in High Resolution')\n",
    "\n",
    "    # Plot 4: Solar Flux per Step\n",
    "    plt.subplot(3, 2, 4)\n",
    "    plt.plot(best_solar_fluxes, label='Solar Flux (W/m^2)')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Solar Flux (W/m^2)')\n",
    "    plt.title(f'{season.capitalize()} - Solar Flux per Step')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot 5: Kite Thrust per Step\n",
    "    plt.subplot(3, 2, 5)\n",
    "    plt.plot(best_kite_thrusts, label='Kite Thrust (N)')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Kite Thrust (N)')\n",
    "    plt.title(f'{season.capitalize()} - Kite Thrust per Step')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot 6: Kite Power per Step\n",
    "    plt.subplot(3, 2, 6)\n",
    "    plt.plot(best_kite_powers, label='Kite Power (W)')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Kite Power (W)')\n",
    "    plt.title(f'{season.capitalize()} - Kite Power per Step')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(season_dir, f'{season}_best_day_plots.png'), dpi=220)  # Save with high resolution\n",
    "    plt.close()\n",
    "\n",
    "    # Save the best path's solar flux and kite power data as npy files\n",
    "    np.save(os.path.join(season_dir, f'{season}_best_solar_fluxes.npy'), best_solar_fluxes)\n",
    "    np.save(os.path.join(season_dir, f'{season}_best_kite_powers.npy'), best_kite_powers)\n",
    "\n",
    "    print(f\"Best day of {season.capitalize()} has reward {best_reward} and is saved in {season_dir}\")\n",
    "\n",
    "def process_seasonal_routes_dqn(agent, env, season, num_episodes=1000):\n",
    "    results_dir = \"seasonal_routes\"\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "\n",
    "    low_res_lat = env.maze.lat\n",
    "    low_res_lon = env.maze.lon\n",
    "    \n",
    "    # Load the high-resolution latitude, longitude, and maze data separately\n",
    "    high_res_lat = np.load('high_res_lat.npy')  # Assuming you saved it as 'high_res_lat.npy'\n",
    "    high_res_lon = np.load('high_res_lon.npy')  # Assuming you saved it as 'high_res_lon.npy'\n",
    "    high_res_maze = np.load('high_res_maze.npy')  # Assuming this is the maze data\n",
    "\n",
    "    print(f\"Processing {season.capitalize()}...\")\n",
    "\n",
    "    season_dir = os.path.join(results_dir, season)\n",
    "    if not os.path.exists(season_dir):\n",
    "        os.makedirs(season_dir)\n",
    "    \n",
    "    best_reward = -float('inf')\n",
    "    best_path = None\n",
    "    best_solar_fluxes = []\n",
    "    best_kite_thrusts = []\n",
    "    best_kite_powers = []\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        episode_reward, episode_step, path, solar_fluxes, kite_thrusts, kite_powers = finish_episode_dqn(agent, env, episode, train=True, optimize_for=\"energy\")\n",
    "        episode_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode} - Steps: {episode_step}, Reward: {episode_reward}\")\n",
    "        if episode % 100 == 0:\n",
    "            clear_output(wait=False)\n",
    "\n",
    "        if episode_reward > best_reward:\n",
    "            best_reward = episode_reward\n",
    "            best_path = path\n",
    "            best_solar_fluxes = solar_fluxes\n",
    "            best_kite_thrusts = kite_thrusts\n",
    "            best_kite_powers = kite_powers\n",
    "\n",
    "    # Plot the best path for the season and save the data\n",
    "    plot_best_path_and_data(season_dir, season, best_path, best_solar_fluxes, best_kite_thrusts, best_kite_powers, best_reward, episode_rewards, low_res_lat, low_res_lon, high_res_lat, high_res_lon, high_res_maze)\n",
    "\n",
    "    # Clear data from memory after processing the season\n",
    "    clear_memory()\n",
    "\n",
    "    print(f\"{season.capitalize()} processing complete and results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.88 GiB for an array with shape (252986224,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m high_res_maze_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh_res_maze.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m new_z \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhigh_res_maze_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize the route agent\u001b[39;00m\n\u001b[0;32m      5\u001b[0m route_agent \u001b[38;5;241m=\u001b[39m QLearningAgent(maze, num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\parsh\\PycharmProjects\\pythonProject6\\.venv\\Lib\\site-packages\\numpy\\lib\\npyio.py:456\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[0;32m    454\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 456\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[1;32mc:\\Users\\parsh\\PycharmProjects\\pythonProject6\\.venv\\Lib\\site-packages\\numpy\\lib\\format.py:809\u001b[0m, in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[0;32m    808\u001b[0m         \u001b[38;5;66;03m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[1;32m--> 809\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    811\u001b[0m         \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[0;32m    822\u001b[0m         array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mndarray(count, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.88 GiB for an array with shape (252986224,) and data type int64"
     ]
    }
   ],
   "source": [
    "high_res_maze_file = 'high_res_maze.npy'\n",
    "new_z = np.load(high_res_maze_file)\n",
    "\n",
    "# Initialize the route agent\n",
    "route_agent = QLearningAgent(maze, num_episodes=1000)\n",
    "\n",
    "# Calculate seasonal averages\n",
    "seasonal_data = calculate_hourly_averages_for_season(2023)\n",
    "\n",
    "# Process seasonal routes\n",
    "high_res_lat = np.load('high_res_lat.npy')\n",
    "high_res_lon = np.load('high_res_lon.npy')\n",
    "high_res_maze = np.load('high_res_maze.npy')\n",
    "\n",
    "env = ShipNavigationEnv(maze, seasonal_data, season='winter')\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(input_dim, output_dim)\n",
    "\n",
    "num_episodes = 1000\n",
    "max_steps_per_episode = 1000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    episode_reward, episode_step, path = finish_episode_dqn(agent, env, max_steps_per_episode, train=True)\n",
    "    print(f\"Episode {episode}: Reward = {episode_reward}, Steps = {episode_step}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.88 GiB for an array with shape (252986224,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 156\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# Load the high-resolution maze once\u001b[39;00m\n\u001b[0;32m    155\u001b[0m high_res_maze_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh_res_maze.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 156\u001b[0m new_z \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhigh_res_maze_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# Initialize the route agent\u001b[39;00m\n\u001b[0;32m    159\u001b[0m route_agent \u001b[38;5;241m=\u001b[39m QLearningAgent(maze, num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\parsh\\PycharmProjects\\pythonProject6\\.venv\\Lib\\site-packages\\numpy\\lib\\npyio.py:456\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[0;32m    454\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 456\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[1;32mc:\\Users\\parsh\\PycharmProjects\\pythonProject6\\.venv\\Lib\\site-packages\\numpy\\lib\\format.py:809\u001b[0m, in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[0;32m    808\u001b[0m         \u001b[38;5;66;03m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[1;32m--> 809\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    811\u001b[0m         \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[0;32m    822\u001b[0m         array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mndarray(count, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.88 GiB for an array with shape (252986224,) and data type int64"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "import gc\n",
    "\n",
    "def plot_best_path_and_data(season_dir, season, best_path, best_solar_fluxes, best_kite_thrusts, best_kite_powers, best_reward, episode_rewards, low_res_lat, low_res_lon, high_res_lat, high_res_lon, high_res_maze):\n",
    "    plt.figure(figsize=(38.4, 21.6), dpi=220)  # 4K resolution: 3840x2160 pixels\n",
    "\n",
    "    # Plot 1: Episode rewards\n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.plot(episode_rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title(f'{season.capitalize()} - Reward per Episode')\n",
    "\n",
    "    # Plot 2: Steps per episode (constant for best path)\n",
    "    plt.subplot(3, 2, 2)\n",
    "    plt.plot([len(best_path)] * len(episode_rewards))\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Steps Taken')\n",
    "    plt.title(f'{season.capitalize()} - Steps per Episode')\n",
    "\n",
    "    # Plot 3: High-resolution maze with best path\n",
    "    plt.subplot(3, 2, 3)\n",
    "    plt.imshow(high_res_maze, cmap='gray')\n",
    "\n",
    "    if best_path is not None:\n",
    "        # Translate low-resolution path to latitude and longitude\n",
    "        translated_path = []\n",
    "        for point in best_path:\n",
    "            lat = low_res_lat[point[1]]\n",
    "            lon = low_res_lon[point[0]]\n",
    "            translated_path.append((lat, lon))\n",
    "        \n",
    "        # Find the closest points on the high-resolution maze\n",
    "        high_res_path = []\n",
    "        for lat, lon in translated_path:\n",
    "            lat_diff = np.abs(high_res_lat - lat)\n",
    "            lon_diff = np.abs(high_res_lon - lon)\n",
    "            closest_lat_idx = np.argmin(lat_diff)\n",
    "            closest_lon_idx = np.argmin(lon_diff)\n",
    "            high_res_path.append((closest_lon_idx, closest_lat_idx))\n",
    "\n",
    "        high_res_path_x, high_res_path_y = zip(*high_res_path)\n",
    "        \n",
    "        # Plot the translated path with a thick line\n",
    "        plt.plot(high_res_path_x, high_res_path_y, color='blue', linewidth=3)  # Thicker line for path\n",
    "\n",
    "        # Plot the start and goal positions\n",
    "        start_lat = low_res_lat[maze.start_position[1]]\n",
    "        start_lon = low_res_lon[maze.start_position[0]]\n",
    "        start_lat_diff = np.abs(high_res_lat - start_lat)\n",
    "        start_lon_diff = np.abs(high_res_lon - start_lon)\n",
    "        start_closest_lat_idx = np.argmin(start_lat_diff)\n",
    "        start_closest_lon_idx = np.argmin(start_lon_diff)\n",
    "        plt.scatter([start_closest_lon_idx], [start_closest_lat_idx], color='red', s=100, label='Start')\n",
    "\n",
    "        goal_lat = low_res_lat[maze.goal_position[1]]\n",
    "        goal_lon = low_res_lon[maze.goal_position[0]]\n",
    "        goal_lat_diff = np.abs(high_res_lat - goal_lat)\n",
    "        goal_lon_diff = np.abs(high_res_lon - goal_lon)\n",
    "        goal_closest_lat_idx = np.argmin(goal_lat_diff)\n",
    "        goal_closest_lon_idx = np.argmin(goal_lon_diff)\n",
    "        plt.scatter([goal_closest_lon_idx], [goal_closest_lat_idx], color='green', s=100, label='Goal')\n",
    "\n",
    "        plt.xticks([]), plt.yticks([])\n",
    "        plt.grid(color='black', linewidth=2)\n",
    "        plt.title(f'{season.capitalize()} - Final Path Learned in High Resolution')\n",
    "\n",
    "    # Plot 4: Solar Flux per Step\n",
    "    plt.subplot(3, 2, 4)\n",
    "    plt.plot(best_solar_fluxes, label='Solar Flux (W/m^2)')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Solar Flux (W/m^2)')\n",
    "    plt.title(f'{season.capitalize()} - Solar Flux per Step')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot 5: Kite Thrust per Step\n",
    "    plt.subplot(3, 2, 5)\n",
    "    plt.plot(best_kite_thrusts, label='Kite Thrust (N)')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Kite Thrust (N)')\n",
    "    plt.title(f'{season.capitalize()} - Kite Thrust per Step')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot 6: Kite Power per Step\n",
    "    plt.subplot(3, 2, 6)\n",
    "    plt.plot(best_kite_powers, label='Kite Power (W)')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Kite Power (W)')\n",
    "    plt.title(f'{season.capitalize()} - Kite Power per Step')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(season_dir, f'{season}_best_day_plots.png'), dpi=220)  # Save with high resolution\n",
    "    plt.close()\n",
    "\n",
    "    # Save the best path's solar flux and kite power data as npy files\n",
    "    np.save(os.path.join(season_dir, f'{season}_best_solar_fluxes.npy'), best_solar_fluxes)\n",
    "    np.save(os.path.join(season_dir, f'{season}_best_kite_powers.npy'), best_kite_powers)\n",
    "\n",
    "    print(f\"Best day of {season.capitalize()} has reward {best_reward} and is saved in {season_dir}\")\n",
    "\n",
    "def process_seasonal_routes(agent, maze, seasonal_data, season, num_episodes=1000):\n",
    "    results_dir = \"seasonal_routes\"\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "\n",
    "    low_res_lat = maze.lat\n",
    "    low_res_lon = maze.lon\n",
    "    \n",
    "    # Load the high-resolution latitude, longitude, and maze data separately\n",
    "    high_res_lat = np.load('high_res_lat.npy')  # Assuming you saved it as 'high_res_lat.npy'\n",
    "    high_res_lon = np.load('high_res_lon.npy')  # Assuming you saved it as 'high_res_lon.npy'\n",
    "    high_res_maze = np.load('high_res_maze.npy')  # Assuming this is the maze data\n",
    "\n",
    "    print(f\"Processing {season.capitalize()}...\")\n",
    "\n",
    "    season_dir = os.path.join(results_dir, season)\n",
    "    if not os.path.exists(season_dir):\n",
    "        os.makedirs(season_dir)\n",
    "    \n",
    "    best_reward = -float('inf')\n",
    "    best_path = None\n",
    "    best_solar_fluxes = []\n",
    "    best_kite_thrusts = []\n",
    "    best_kite_powers = []\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        episode_reward, episode_step, path, solar_fluxes, kite_thrusts, kite_powers = finish_episode_with_hourly_averages(agent, maze, episode, seasonal_data, season, train=True, optimize_for=\"energy\")\n",
    "        episode_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode} - Steps: {episode_step}, Reward: {episode_reward}\")\n",
    "        if episode % 100 == 0:\n",
    "            clear_output(wait=False)\n",
    "\n",
    "        if episode_reward > best_reward:\n",
    "            best_reward = episode_reward\n",
    "            best_path = path\n",
    "            best_solar_fluxes = solar_fluxes\n",
    "            best_kite_thrusts = kite_thrusts\n",
    "            best_kite_powers = kite_powers\n",
    "\n",
    "    # Plot the best path for the season and save the data\n",
    "    plot_best_path_and_data(season_dir, season, best_path, best_solar_fluxes, best_kite_thrusts, best_kite_powers, best_reward, episode_rewards, low_res_lat, low_res_lon, high_res_lat, high_res_lon, high_res_maze)\n",
    "\n",
    "    # Clear data from memory after processing the season\n",
    "    clear_memory()\n",
    "\n",
    "    print(f\"{season.capitalize()} processing complete and results saved.\")\n",
    "\n",
    "# Example usage:\n",
    "# Load the high-resolution maze once\n",
    "high_res_maze_file = 'high_res_maze.npy'\n",
    "new_z = np.load(high_res_maze_file)\n",
    "\n",
    "# Initialize the route agent\n",
    "route_agent = QLearningAgent(maze, num_episodes=1000)\n",
    "\n",
    "# Calculate seasonal averages\n",
    "seasonal_data = calculate_hourly_averages_for_season(2023)\n",
    "\n",
    "# Process seasonal routes\n",
    "high_res_lat = np.load('high_res_lat.npy')\n",
    "high_res_lon = np.load('high_res_lon.npy')\n",
    "high_res_maze = np.load('high_res_maze.npy')\n",
    "\n",
    "# Process the routes\n",
    "# Example usage for winter\n",
    "process_seasonal_routes(agent=route_agent, maze=maze, seasonal_data=seasonal_data, season='winter', num_episodes=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
