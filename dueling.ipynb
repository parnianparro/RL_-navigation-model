{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda, Add, Subtract\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from global_land_mask import globe\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from netCDF4 import Dataset\n",
    "from datetime import datetime, timedelta\n",
    "from global_land_mask import globe\n",
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scipy.interpolate import splprep, splev\n",
    "import multiprocessing as mp\n",
    "from tensorflow.keras import backend as K\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import tensorflow as tf\n",
    "RECTANGLE_COORDS = [46.248, 19.859, 64.881, 32.955]\n",
    "SOLAR_PANEL_AREA = 3000\n",
    "SOLAR_PANEL_EFFICIENCY = 0.1726\n",
    "RHO_AIR = 1.225\n",
    "KITE_AREA = 500\n",
    "LIFT_COEFFICIENT = 1.0\n",
    "DRAG_COEFFICIENT = 0.1\n",
    "SHIP_SPEED = 10.29  # m/s\n",
    "GRID_SIZE_DEGREES = 0.01\n",
    "MAX_STEPS_PER_EPISODE = 3945\n",
    "TIME_INCREMENT_MINUTES = 2.22*60\n",
    "goal_reward = 1000000\n",
    "wall_penalty = -20000\n",
    "energy_consumption_penalty = -100\n",
    "\n",
    "# Directories for wind and solar data\n",
    "WIND_DATA_DIR = \"mera\"\n",
    "SOLAR_DATA_DIR = \"solar energy\"\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.priorities = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, error, transition):\n",
    "        priority = (np.abs(error) + 1e-5) ** self.alpha\n",
    "        self.buffer.append(transition)\n",
    "        self.priorities.append(priority)\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        scaled_priorities = np.array(self.priorities) ** self.alpha\n",
    "        sample_probs = scaled_priorities / np.sum(scaled_priorities)\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=sample_probs)\n",
    "        samples = np.array(self.buffer)[indices]\n",
    "        weights = (len(self.buffer) * sample_probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        return samples, indices, weights\n",
    "\n",
    "    def update_priorities(self, indices, errors):\n",
    "        for i, error in zip(indices, errors):\n",
    "            self.priorities[i] = (np.abs(error) + 1e-5) ** self.alpha\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DoubleDuelingDQNAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.001, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, batch_size=64, update_target_frequency=1000):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.update_target_frequency = update_target_frequency\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "        self.memory = PrioritizedReplayBuffer(2000)\n",
    "        self.steps = 0\n",
    "\n",
    "    def _build_model(self):\n",
    "        state_input = Input(shape=(self.state_size,))\n",
    "        hidden1 = Dense(64, activation='relu')(state_input)\n",
    "        hidden2 = Dense(64, activation='relu')(hidden1)\n",
    "\n",
    "        value_fc = Dense(64, activation='relu')(hidden2)\n",
    "        value = Dense(1, activation='linear')(value_fc)\n",
    "\n",
    "        advantage_fc = Dense(64, activation='relu')(hidden2)\n",
    "        advantage = Dense(self.action_size, activation='linear')(advantage_fc)\n",
    "\n",
    "        advantage_mean = Lambda(lambda x: tf.reduce_mean(x, axis=1, keepdims=True))(advantage)\n",
    "        advantage_centered = Subtract()([advantage, advantage_mean])\n",
    "        q_values = Add()([value, advantage_centered])\n",
    "\n",
    "        model = Model(inputs=state_input, outputs=q_values)\n",
    "        model.compile(optimizer=Adam(learning_rate= 0.01), loss='mse')\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        next_state = np.reshape(next_state, [1, self.state_size])\n",
    "        target = reward\n",
    "        if not done:\n",
    "            target += self.gamma * np.amax(self.target_model.predict(next_state, verbose=0)[0])\n",
    "        current_q = self.model.predict(state, verbose=0)[0][action]\n",
    "        error = abs(current_q - target)\n",
    "        self.memory.add(error, (state, action, reward, next_state, done))\n",
    "\n",
    "    def train(self, beta=0.4):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch, indices, weights = self.memory.sample(self.batch_size, beta)\n",
    "        states = np.concatenate([x[0] for x in minibatch])\n",
    "        actions = np.array([x[1] for x in minibatch])\n",
    "        rewards = np.array([x[2] for x in minibatch])\n",
    "        next_states = np.concatenate([x[3] for x in minibatch])\n",
    "        dones = np.array([x[4] for x in minibatch])\n",
    "\n",
    "        target_q_values = self.model.predict(states, verbose=0)\n",
    "        next_model_q_values = self.model.predict(next_states, verbose=0)\n",
    "        next_target_q_values = self.target_model.predict(next_states, verbose=0)\n",
    "        max_next_actions = np.argmax(next_model_q_values, axis=1)\n",
    "        targets = rewards + (1 - dones) * self.gamma * next_target_q_values[np.arange(self.batch_size), max_next_actions]\n",
    "\n",
    "        errors = np.abs(target_q_values[np.arange(self.batch_size), actions] - targets)\n",
    "        for i in range(self.batch_size):\n",
    "            target_q_values[i, actions[i]] = targets[i]\n",
    "\n",
    "        self.memory.update_priorities(indices, errors)\n",
    "        self.model.fit(states, target_q_values, epochs=1, verbose=0, sample_weight=weights)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        self.steps += 1\n",
    "        if self.steps % self.update_target_frequency == 0:\n",
    "            self.update_target_model()\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "class Maze:\n",
    "    def __init__(self, maze, start_position, goal_position, height, width, lat, lon):\n",
    "        self.maze = maze\n",
    "        self.maze_height = height\n",
    "        self.maze_width = width\n",
    "        self.start_position = start_position\n",
    "        self.goal_position = goal_position\n",
    "        self.lat = lat\n",
    "        self.lon = lon\n",
    "\n",
    "    def show_maze(self):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(self.maze, cmap='gray')\n",
    "        plt.text(self.start_position[0], self.start_position[1], 'S', ha='center', va='center', color='red', fontsize=20)\n",
    "        plt.text(self.goal_position[0], self.goal_position[1], 'G', ha='center', va='center', color='green', fontsize=20)\n",
    "        plt.xticks([]), plt.yticks([])\n",
    "        plt.show()\n",
    "\n",
    "# Helper functions for the environment and training\n",
    "def load_and_preprocess_data(date, directory, variable):\n",
    "    filename = f\"MERRA2_400.tavg1_2d_flx_Nx.{date.strftime('%Y%m%d')}.nc4.nc4\" if variable in ['ULML', 'VLML'] else f\"MERRA2_400.tavg1_2d_rad_Nx.{date.strftime('%Y%m%d')}.nc4_2.nc4\"\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    dataset = Dataset(filepath)\n",
    "    data = dataset.variables[variable][:]\n",
    "    return data\n",
    "\n",
    "def load_and_preprocess_data_once(dates, directory, variables):\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    results = pool.starmap(load_and_preprocess_data, [(date, directory, variable) for date in dates for variable in variables])\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Organize results into a dictionary\n",
    "    data = {var: [] for var in variables}\n",
    "    for i, variable in enumerate(variables):\n",
    "        data[variable] = results[i::len(variables)]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def load_data_parallel(dates, directory, variables):\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    results = pool.starmap(load_and_preprocess_data, [(date, directory, variable) for date in dates for variable in variables])\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return results\n",
    "\n",
    "def calculate_apparent_wind(uwind, vwind, ship_speed, ship_direction):\n",
    "    wind_speed = np.sqrt(uwind**2 + vwind**2)\n",
    "    wind_direction = np.arctan2(vwind, uwind)\n",
    "    apparent_wind_speed = np.sqrt((wind_speed * np.cos(wind_direction - ship_direction) - ship_speed)**2 + (wind_speed * np.sin(wind_direction - ship_direction))**2)\n",
    "    apparent_wind_direction = np.arctan2(wind_speed * np.sin(wind_direction - ship_direction), wind_speed * np.cos(wind_direction - ship_direction) - ship_speed)\n",
    "    return apparent_wind_speed, apparent_wind_direction\n",
    "\n",
    "# Calculate kite power\n",
    "def calculate_kite_power(apparent_wind_speed, kite_area, lift_coefficient, drag_coefficient):\n",
    "    lift_force = 0.5 * RHO_AIR * apparent_wind_speed**2 * kite_area * lift_coefficient\n",
    "    drag_force = 0.5 * RHO_AIR * apparent_wind_speed**2 * kite_area * drag_coefficient\n",
    "    power_output = (lift_force - drag_force) * apparent_wind_speed\n",
    "    return power_output\n",
    "\n",
    "# Calculate solar power output\n",
    "def calculate_solar_power_output(solar_panel_area, solar_panel_efficiency, swgnt):\n",
    "    power_output = solar_panel_area * solar_panel_efficiency * swgnt\n",
    "    return power_output\n",
    "\n",
    "# Calculate the Haversine distance\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Radius of the Earth in kilometers\n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dphi / 2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "def get_state(maze, current_position, goal_position):\n",
    "    distance_to_goal = haversine_distance(\n",
    "        maze.lat[current_position[1]], maze.lon[current_position[0]],\n",
    "        maze.lat[goal_position[1]], maze.lon[goal_position[0]]\n",
    "    )\n",
    "    state = np.array([current_position[0], current_position[1], distance_to_goal])\n",
    "    return state\n",
    "\n",
    "import time\n",
    "\n",
    "def finish_episode(agent, maze, current_episode, start_time, wind_data, solar_data, train=True, optimization_goal='distance'):\n",
    "    start_time_episode = time.time()\n",
    "    current_state = maze.start_position\n",
    "    is_done = False\n",
    "    episode_reward = 0\n",
    "    episode_step = 0\n",
    "    path = [current_state]\n",
    "    solar_fluxes = []\n",
    "    kite_powers = []\n",
    "\n",
    "    time_increment = timedelta(minutes=TIME_INCREMENT_MINUTES)\n",
    "    current_time = start_time\n",
    "\n",
    "    while not is_done and episode_step < MAX_STEPS_PER_EPISODE:\n",
    "        state = np.reshape(get_state(maze, current_state, maze.goal_position), [1, agent.state_size])\n",
    "        action = agent.act(state)\n",
    "        next_state = (current_state[0] + actions[action][0], current_state[1] + actions[action][1])\n",
    "\n",
    "        if (0 <= next_state[0] < maze.maze_width) and (0 <= next_state[1] < maze.maze_height):\n",
    "            if maze.maze[next_state[1], next_state[0]] == 1:\n",
    "                reward = wall_penalty\n",
    "                next_state = current_state\n",
    "            elif next_state == maze.goal_position:\n",
    "                path.append(next_state)\n",
    "                reward = goal_reward - episode_step  # Reward for reaching the goal earlier\n",
    "                is_done = True\n",
    "            else:\n",
    "                path.append(next_state)\n",
    "                lat1, lon1 = maze.lat[current_state[1]], maze.lon[current_state[0]]\n",
    "                lat2, lon2 = maze.lat[next_state[1]], maze.lon[next_state[0]]\n",
    "                lat_end, lon_end = maze.lat[maze.goal_position[1]], maze.lon[maze.goal_position[0]]\n",
    "                distance_reward = -haversine_distance(lat1, lon1, lat2, lon2) - haversine_distance(lat2, lon2, lat_end, lon_end)\n",
    "\n",
    "                # Calculate wind and solar effects\n",
    "                lat_idx, lon_idx = int((lat1 - RECTANGLE_COORDS[1]) / 0.5), int((lon1 - RECTANGLE_COORDS[0]) / 0.625)\n",
    "                uwind = wind_data['uwind'][:, lat_idx, lon_idx]\n",
    "                vwind = wind_data['vwind'][:, lat_idx, lon_idx]\n",
    "                swgnt = solar_data['swgnt'][:, lat_idx, lon_idx]\n",
    "\n",
    "                apparent_wind_speed, apparent_wind_direction = calculate_apparent_wind(uwind, vwind, SHIP_SPEED, np.arctan2(lon2 - lon1, lat2 - lat1))\n",
    "                kite_power = calculate_kite_power(apparent_wind_speed, KITE_AREA, LIFT_COEFFICIENT, DRAG_COEFFICIENT)\n",
    "                solar_power_output = calculate_solar_power_output(SOLAR_PANEL_AREA, SOLAR_PANEL_EFFICIENCY, swgnt)\n",
    "\n",
    "                # Calculate energy consumption\n",
    "                energy_gain = kite_power.mean() + solar_power_output.mean()\n",
    "                energy_reward = energy_consumption_penalty * energy_gain\n",
    "\n",
    "                if optimization_goal == 'distance':\n",
    "                    reward = distance_reward\n",
    "                elif optimization_goal == 'energy':\n",
    "                    reward = energy_reward\n",
    "                else:\n",
    "                    reward = distance_reward + energy_reward\n",
    "\n",
    "                # Add a small step penalty to encourage faster goal reaching\n",
    "                step_penalty = -1\n",
    "                reward += step_penalty\n",
    "\n",
    "                # Record values for plotting later\n",
    "                solar_fluxes.append(solar_power_output.mean())\n",
    "                kite_powers.append(kite_power.mean())\n",
    "\n",
    "                # Update time\n",
    "                current_time += time_increment\n",
    "\n",
    "        else:\n",
    "            reward = wall_penalty\n",
    "            next_state = current_state\n",
    "\n",
    "        episode_reward += reward\n",
    "        episode_step += 1\n",
    "\n",
    "        if train:\n",
    "            next_state_reshaped = np.reshape(get_state(maze, next_state, maze.goal_position), [1, agent.state_size])\n",
    "            agent.remember(state, action, reward, next_state_reshaped, is_done)\n",
    "            if len(agent.memory) >= agent.batch_size:\n",
    "                agent.train()\n",
    "\n",
    "        current_state = next_state\n",
    "\n",
    "    if not is_done:\n",
    "        print(f\"Episode {current_episode + 1} reached max steps without reaching the goal.\")\n",
    "\n",
    "    end_time_episode = time.time()\n",
    "    print(f\"Episode duration: {end_time_episode - start_time_episode} seconds\")\n",
    "\n",
    "    return episode_reward, episode_step, path, solar_fluxes, kite_powers\n",
    "\n",
    "def train_agent(agent, maze, start_time, wind_data, solar_data, num_episodes=100, optimization_goal='distance'):\n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "    best_path = []\n",
    "    best_solar_fluxes = []\n",
    "    best_kite_powers = []\n",
    "    best_reward = -float('inf')\n",
    "    start_training_time = time.time()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        episode_start_time = time.time()\n",
    "\n",
    "        episode_reward, episode_step, path, solar_fluxes, kite_powers = finish_episode(agent, maze, episode, start_time, wind_data, solar_data, train=True, optimization_goal=optimization_goal)\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_steps.append(episode_step)\n",
    "\n",
    "        if episode_reward > best_reward:\n",
    "            best_reward = episode_reward\n",
    "            best_path = path\n",
    "            best_solar_fluxes = solar_fluxes\n",
    "            best_kite_powers = kite_powers\n",
    "\n",
    "        # Print progress every episode\n",
    "        print(f\"Episode {episode + 1} - Steps: {episode_step}, Reward: {episode_reward}\")\n",
    "\n",
    "        # Estimate and print remaining time\n",
    "        episode_end_time = time.time()\n",
    "        episode_duration = episode_end_time - episode_start_time\n",
    "        elapsed_time = episode_end_time - start_training_time\n",
    "        remaining_time = (num_episodes - (episode + 1)) * (elapsed_time / (episode + 1))\n",
    "\n",
    "        print(f\"Estimated time to finish: {remaining_time / 60:.2f} minutes\")\n",
    "        print(f\"Episode duration: {episode_duration} seconds\")\n",
    "\n",
    "        agent.adjust_epsilon(episode)\n",
    "\n",
    "    print(f\"Best Reward: {best_reward}\")\n",
    "    print(f\"Best Path: {best_path}\")\n",
    "    print(f\"Best Solar Fluxes: {best_solar_fluxes}\")\n",
    "    print(f\"Best Kite Powers: {best_kite_powers}\")\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(episode_rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title('Reward per Episode')\n",
    "    average_reward = sum(episode_rewards) / len(episode_rewards)\n",
    "    print(f\"The average reward is: {average_reward}\")\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(episode_steps)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Steps Taken')\n",
    "    plt.title('Steps per Episode')\n",
    "    average_steps = sum(episode_steps) / len(episode_steps)\n",
    "    print(f\"The average steps is: {average_steps}\")\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.imshow(maze.maze, cmap='gray')\n",
    "    plt.text(maze.start_position[0], maze.start_position[1], 'S', ha='center', va='center', color='red', fontsize=20)\n",
    "    plt.text(maze.goal_position[0], maze.goal_position[1], 'G', ha='center', va='center', color='green', fontsize=20)\n",
    "    for row, col in best_path:\n",
    "        plt.text(row, col, \"x\", va='center', color='blue', fontsize=5)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.grid(color='black', linewidth=2)\n",
    "    plt.title('Final Path Learned')\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(best_solar_fluxes, label='Solar Flux (W/m^2)')\n",
    "    plt.plot(best_kite_powers, label='Kite Power (W)')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Solar Flux and Kite Power per Step')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Initialize environment and agent\n",
    "origin = [49.469, 29.362]\n",
    "destination = [58.836, 24.385]\n",
    "if origin[0] > destination[0]:\n",
    "    origin, destination = destination, origin\n",
    "\n",
    "lat = np.arange(19.859, 32.955, 0.5)\n",
    "lon = np.arange(46.248, 64.881+0.625,0.625 )\n",
    "lon_grid, lat_grid = np.meshgrid(lon, lat)\n",
    "z = globe.is_land(lat_grid, lon_grid)\n",
    "z = np.array(z[::-1].astype(int))\n",
    "\n",
    "test_lat = lat - origin[1]\n",
    "test_lat = test_lat[::-1]\n",
    "test_lon = lon - origin[0]\n",
    "start_pos_lat = np.where(abs(test_lat) == min(abs(test_lat)))\n",
    "start_pos_lon = np.where(abs(test_lon) == min(abs(test_lon)))\n",
    "startpos = (start_pos_lon[0][0], start_pos_lat[0][0])\n",
    "\n",
    "test_lat = lat - destination[1]\n",
    "test_lat = test_lat[::-1]\n",
    "test_lon = lon - destination[0]\n",
    "destination_pos_lat = np.where(abs(test_lat) == min(abs(test_lat)))\n",
    "destination_pos_lon = np.where(abs(test_lon) == min(abs(test_lon)))\n",
    "destinationpos = (destination_pos_lon[0][0], destination_pos_lat[0][0])\n",
    "lat = lat[::-1]\n",
    "lon = lon[::-1]\n",
    "\n",
    "maze = Maze(z, startpos, destinationpos, len(z), len(z[0]), lat, lon)\n",
    "maze.show_maze()\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1), (1, 1), (1, -1), (-1, 1), (-1, -1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state_size = 3\n",
    "action_size = len(actions)\n",
    "num_episodes = 1000\n",
    "start_time = datetime(2023, 1, 1)\n",
    "\n",
    "agent = DoubleDuelingDQNAgent(state_size, action_size, num_episodes)\n",
    "\n",
    "# Load wind and solar data\n",
    "# Load wind and solar data once before training\n",
    "dates = [start_time + timedelta(days=i) for i in range(num_episodes)]\n",
    "wind_data = {\n",
    "    'uwind': load_and_preprocess_data(start_time, WIND_DATA_DIR, 'ULML'),\n",
    "    'vwind': load_and_preprocess_data(start_time, WIND_DATA_DIR, 'VLML')\n",
    "}\n",
    "solar_data = {\n",
    "    'swgnt': load_and_preprocess_data(start_time, SOLAR_DATA_DIR, 'SWGNT')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (64, 5) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Training the agent\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaze\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwind_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolar_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimization_goal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menergy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 355\u001b[0m, in \u001b[0;36mtrain_agent\u001b[1;34m(agent, maze, start_time, wind_data, solar_data, num_episodes, optimization_goal)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m    353\u001b[0m     episode_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 355\u001b[0m     episode_reward, episode_step, path, solar_fluxes, kite_powers \u001b[38;5;241m=\u001b[39m \u001b[43mfinish_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaze\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwind_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolar_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimization_goal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimization_goal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    356\u001b[0m     episode_rewards\u001b[38;5;241m.\u001b[39mappend(episode_reward)\n\u001b[0;32m    357\u001b[0m     episode_steps\u001b[38;5;241m.\u001b[39mappend(episode_step)\n",
      "Cell \u001b[1;32mIn[11], line 331\u001b[0m, in \u001b[0;36mfinish_episode\u001b[1;34m(agent, maze, current_episode, start_time, wind_data, solar_data, train, optimization_goal)\u001b[0m\n\u001b[0;32m    329\u001b[0m         agent\u001b[38;5;241m.\u001b[39mremember(state, action, reward, next_state_reshaped, is_done)\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agent\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[1;32m--> 331\u001b[0m             \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    333\u001b[0m     current_state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_done:\n",
      "Cell \u001b[1;32mIn[11], line 137\u001b[0m, in \u001b[0;36mDoubleDuelingDQNAgent.train\u001b[1;34m(self, beta)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m minibatch, indices, weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m states \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m minibatch])\n\u001b[0;32m    139\u001b[0m actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m minibatch])\n",
      "Cell \u001b[1;32mIn[11], line 63\u001b[0m, in \u001b[0;36mPrioritizedReplayBuffer.sample\u001b[1;34m(self, batch_size, beta)\u001b[0m\n\u001b[0;32m     61\u001b[0m sample_probs \u001b[38;5;241m=\u001b[39m scaled_priorities \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(scaled_priorities)\n\u001b[0;32m     62\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer), batch_size, p\u001b[38;5;241m=\u001b[39msample_probs)\n\u001b[1;32m---> 63\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m[indices]\n\u001b[0;32m     64\u001b[0m weights \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer) \u001b[38;5;241m*\u001b[39m sample_probs[indices]) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39mbeta)\n\u001b[0;32m     65\u001b[0m weights \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39mmax()\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (64, 5) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Training the agent\n",
    "train_agent(agent, maze, start_time, wind_data, solar_data, num_episodes=num_episodes, optimization_goal='energy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
